# [ECCV 2024] CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning 

### by [Ziyang Gong](https://scholar.google.com/citations?user=cWip8QgAAAAJ&hl=zh-CN&oi=ao), [Fuhao Li](https://scholar.google.com/citations?user=UGfKnL8AAAAJ&hl=zh-CN&oi=ao), [Yupeng Deng](https://scholar.google.com/citations?user=H5X8NDQAAAAJ&hl=zh-CN&oi=ao), [Deblina Bhattacharjee](https://scholar.google.com/citations?user=F3YYEmMAAAAJ&hl=zh-CN&oi=ao), [Xianzheng Ma](https://scholar.google.com/citations?hl=zh-CN&user=NS8g2mMAAAAJ), [Xiangwei Zhu](), [Zhenming Ji](https://scholar.google.com/citations?hl=zh-CN&user=Hp80uDwAAAAJ)

<div align="center">
  <table>
    <tr>
      <td align="center">
        <img src="images/sysu_logo.png" width="150px" height="150px"><br>
        <span style="font-size: 18px;"><b>Sun Yat-sen University</b></span>
      </td>
      <td align="center">
        <img src="images/CPNT_logo.png" width="150px" height="150px"><br>
        <span style="font-size: 18px;"><b>CPNT Lab</b></span>
      </td>
      <td align="center">
        <img src="images/EPFL_logo.png" width="150px" height="150px"><br>
        <span style="font-size: 18px;"><b>EPFL</b></span>
      </td>
    </tr>
  </table>
</div>


## üåüüåüüåü Home
Here is the official project of üéª[CoDA](). We are releasing the training code and dataset generated by ourselves in our paper.

CoDA is a UDA methodology that boosts models to understand all adverse scenes (‚òÅÔ∏è,‚òî,‚ùÑÔ∏è,&#x1F319;) by highlighting the discrepancies between and within these scenes.
CoDA achieves state-of-the-art performances on widely used benchmarks.
## üî•üî•üî• News
[2024-7-10]We have released our generated data samples. You can download from here. 

[<ins>[Baidu Netdisk]</ins>](https://pan.baidu.com/s/1TLp6nvvzXKh-4E8c2Cd7dg?pwd=ekw9 )&nbsp;&nbsp;&nbsp;&nbsp;[<ins>[Google Drive]</ins>](https://drive.google.com/drive/folders/1UfCVIO0H4MGdEZTHCnwfSd3rkfBfJRu7?usp=sharing)

[2024-7-2] We are delighted to inform that CoDA has been accepted by ECCV 2024 main conference üéâüéâüéâ!!!

[2024-3-8] We create the official project of CoDA and release the inference code.

## Overview

![night](images/demo1.png)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/coda-instructive-chain-of-domain-adaptation/domain-adaptation-on-cityscapes-to)](https://paperswithcode.com/sota/domain-adaptation-on-cityscapes-to?p=coda-instructive-chain-of-domain-adaptation)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/coda-instructive-chain-of-domain-adaptation/domain-adaptation-on-cityscapes-to-1)](https://paperswithcode.com/sota/domain-adaptation-on-cityscapes-to-1?p=coda-instructive-chain-of-domain-adaptation)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/coda-instructive-chain-of-domain-adaptation/domain-adaptation-on-cityscapes-to-acdc)](https://paperswithcode.com/sota/domain-adaptation-on-cityscapes-to-acdc?p=coda-instructive-chain-of-domain-adaptation)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/coda-instructive-chain-of-domain-adaptation/semantic-segmentation-on-nighttime-driving)](https://paperswithcode.com/sota/semantic-segmentation-on-nighttime-driving?p=coda-instructive-chain-of-domain-adaptation)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/coda-instructive-chain-of-domain-adaptation/semantic-segmentation-on-dark-zurich)](https://paperswithcode.com/sota/semantic-segmentation-on-dark-zurich?p=coda-instructive-chain-of-domain-adaptation)
<a href="" target='_blank'><img src="https://visitor-badge.laobi.icu/badge?page_id=Cuzyoung.CoDA&left_color=%23DFA3CB&right_color=%23CEE75F"> </a> 
<!-- 
 ![visitors](https://visitor-badge.glitch.me/badge?page_id=Cuzyoung.CoDA&left_color=%23DFA3CB&right_color=%23CEE75F) -->

![CoDA](images/Architec.png)

| Experiments | mIoU | Checkpoint |
|-|-|-|
|**Cityscapes $\rightarrow$ ACDC**|**72.6**|-|
|**Cityscapes $\rightarrow$ Foggy Zurich**|**60.9**|-|
|**Cityscapes $\rightarrow$ Foggy Driving**|**61.0**|-|
|**Cityscapes $\rightarrow$ Dark Zurich**|**61.2**|-|
|**Cityscapes $\rightarrow$ Nighttime Driving**|**59.2**|-|
|**Cityscapes $\rightarrow$ BDD100K-Night**|**41.6**|-|

If you find this project useful in your research, please consider citing:
```
@article{gong2024coda,
  title={CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning},
  author={Gong, Ziyang and Li, Fuhao and Deng, Yupeng and Bhattacharjee, Deblina and Zhu, Xiangwei and Ji, Zhenming},
  journal={arXiv preprint arXiv:2403.17369},
  year={2024}
}
```


## Download Checkpoint
```bash
cd CoDA
python ./tools/download_ck.py
```
or you can manually download checkpoints from [Google Drive](https://drive.google.com/drive/folders/1NKfgJZtLGXpqs7zKvI8KpKpJmTYCRtyB?usp=drive_link).

## Environment
```
conda create -n coda python=3.8.5 pip=22.3.1
conda activate coda
pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html
pip install mmcv-full==1.3.7 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7/index.html
```
Before run demo, first configure the PYTHONPATH, or you may encounter error like 'can not found tools...'.
```bash
cd CoDA
export PYTHONPATH=.:$PYTHONPATH
```
or directly modify the .bashrc file
```bash
vi ~/.bashrc
export PYTHONPATH=your path/CoDA:$PYTHONPATH
source ~/.bashrc
```

## demo
```bash
python ./tools/image_demo.py --img ./images/night_demo.png --config ./configs/coda/csHR2acdcHR_coda.py --checkpoint ./pretrained/CoDA_cs2acdc.pth
```
## Inference Steps
```bash
python ./tools/image_demo.py --img_dir ./acdc_dir --config ./configs/coda/csHR2acdcHR_coda.py --checkpoint ./pretrained/CoDA_cs2acdc.pth --out_dir ./workdir/cs2acdc
```
## Traning Steps
```bash
python ./tools/train.py --config ./configs/coda/csHR2acdcHR_coda.py --work-dir ./workdir/cs2acdc
```
